### 2024.11.11-11.15

001 对数据库vip_vuln中的三要素（厂商，商品，三要素）进行提取，并且写入json文件。

002 分别使用闭源Qwen72B(4位，经过量化的) 和Qwen 14B（16位）和prompt engineering，对json文件中的三要素生成cpe, 然后进行对比。

002 提取数据表vip_cpe 的 cpe_item_name字段，转为Json格式，再将其向量化（embedding)，然后将向量数据和原始数据存入milvus向量数据库。

003 在服务器10.0.81.159部署Qwen2.5-14B-Instruct模型，在本地进行测试。

### 2024.11.18-11.22

001 使用gradio实现前后端交互，前端传入原始的漏洞情报，然后输出从向量数据库中匹配的Top20相似的7要素，即"part", "vendor", "product", "version", "update", "edition", "language"，score(使用cosine余弦相似度检索)。

002 使用FastAPI 对实现对漏洞情报的处理，即1.vip_cpe数据存储接口（@app.post("/input_cpe_list")）2. 大模型提取cpe接口(@app.post("/extract_cpe")) 3.cpe字符串与mivlus向量数据库匹配接口(@app.post("/search_cpe"))

003 to_do list:如何确保在search的时候，比如两个输入[
    "cpe:/a:mitsubishi_electric:mc_works64:*:*:*:*",
    "cpe:/a:iconics:genesis64:*:*:*:*"
  ] 怎么才能返回与他们最相似的前两个相似度 ，而不是一个先返回，然后再返回另一个。

### 2024.11.25-11.29

001 使用"name" "author"自我认知相关的数据，对大模型的输出角色进行微调，完成大模型自我认知的Fine-Tuning，**遇到的问题：微调之后大模型对通用语料的解决能力急剧下降！。** 加通用预料进去 或者数据格式不要那么的单一！

002 使用GPT4o API KEY对英文多轮对话数据进行翻译。

# **怎么进行英文多轮对话的翻译？**

对于翻译任务，使用多进程还是多线程取决于以下几个因素：

### 1. **API的性质**：

- **I/O密集型任务**：如果翻译任务主要依赖于网络请求和响应（即调用外部翻译API），那么它属于**I/O密集型**任务。这类任务通常受限于网络延迟，而不是CPU计算能力。**因此，多线程通常是处理I/O密集型任务的更好选择。**
- **CPU密集型任务**：如果你的任务涉及大量的计算或数据处理（如文本分析、模型推理等），那么使用**多进程**更合适，因为多进程可以充分利用多个CPU核心，避免GIL（全局解释器锁）限制。

### 2. **Python的多线程与多进程区别**：

- **多线程**：
  - 多线程在 Python 中主要通过共享内存进行协作。由于 Python 的全局解释器锁（GIL），即使使用多个线程，它们仍然只能在一个CPU核心上并行执行Python字节码。因此，多线程更适合处理I/O密集型任务（如网络请求），因为GIL在等待I/O操作时会释放。
  - 例如，调用外部API时，线程会等待API返回的结果，其他线程可以在此期间继续发送请求，这样能提高效率。
- **多进程**：
  - 多进程能够利用多个CPU核心，每个进程有自己独立的内存空间，不受GIL限制。对于需要大量计算的任务（如大规模的数据处理、数值计算等），使用多进程能显著提高效率。
  - 如果任务是计算密集型的，或者你的翻译工作涉及很多数据预处理或后处理操作，可以使用多进程来加速处理。

### 3. **如何选择**：

- **API调用翻译**属于典型的I/O密集型任务，所以通常使用**多线程**会更高效。你可以使用 `concurrent.futures.ThreadPoolExecutor` 或 `asyncio` 来实现多线程并行。
- 如果你不确定，或者你的任务包含大量数据预处理或其他计算密集型操作，使用**多进程**也是一种选择。`concurrent.futures.ProcessPoolExecutor` 适用于多进程并行。

### 4. **实现示例**：

#### **使用多线程 (ThreadPoolExecutor)**

```python
from concurrent.futures import ThreadPoolExecutor
import requests

def translate_text(text):
    # 假设你使用一个API进行翻译
    response = requests.post('http://example.com/translate', data={'text': text})
    return response.json()['translated_text']

def translate_batch(texts):
    with ThreadPoolExecutor(max_workers=10) as executor:
        translated_texts = list(executor.map(translate_text, texts))
    return translated_texts

# 示例：翻译一批文本
texts = ["Hello world", "How are you?", "Good morning"]
translated_texts = translate_batch(texts)
print(translated_texts)
```

#### **使用多进程 (ProcessPoolExecutor)**

```python
from concurrent.futures import ProcessPoolExecutor
import requests

def translate_text(text):
    # 假设你使用一个API进行翻译
    response = requests.post('http://example.com/translate', data={'text': text})
    return response.json()['translated_text']

def translate_batch(texts):
    with ProcessPoolExecutor(max_workers=4) as executor:
        translated_texts = list(executor.map(translate_text, texts))
    return translated_texts

# 示例：翻译一批文本
texts = ["Hello world", "How are you?", "Good morning"]
translated_texts = translate_batch(texts)
print(translated_texts)
```

### 5. **使用 `asyncio`（异步）**：

如果API支持异步请求，可以使用 `asyncio` 和 `aiohttp` 来进行并发处理，这样可以充分利用I/O等待时间，提升效率。

```python
import asyncio
import aiohttp

async def translate_text(session, text):
    async with session.post('http://example.com/translate', data={'text': text}) as response:
        result = await response.json()
        return result['translated_text']

async def translate_batch(texts):
    async with aiohttp.ClientSession() as session:
        tasks = [translate_text(session, text) for text in texts]
        translated_texts = await asyncio.gather(*tasks)
    return translated_texts

# 示例：翻译一批文本
texts = ["Hello world", "How are you?", "Good morning"]
translated_texts = asyncio.run(translate_batch(texts))
print(translated_texts)
```

### 6. **总结**：

- **多线程**适用于API调用等I/O密集型任务，能够更高效地处理多个并发请求。
- **多进程**适用于CPU密集型任务，如计算、数据处理等，能够利用多个核心。
- **异步编程**（如 `asyncio`）是处理高并发I/O任务的一个高效选择，特别适合API调用。

对于你提到的翻译任务，建议你先使用**多线程**或**异步编程**，如果你感觉性能不够，再考虑使用多进程。

### To do list

001 Qwen7B 大模型安全对齐测试，使用https://www.modelscope.cn/datasets/Shanghai_AI_Laboratory/SafeMTData/dataPeview中的数据集对llm 进行微调, 然后使用 /root/data/wjy/vip_vul_pro/Fine_tune/SafeMTData/Attack_600.json 600个数据进行测试

002 使用Llama-Guard3进行对大模型安全对齐性能的评估

### 2024.12.2-2024.12.6

001 Qwen7B 大模型安全对齐测试，使用https://www.modelscope.cn/datasets/Shanghai_AI_Laboratory/SafeMTData/dataPeview中的数据集对llm 进行微调, 先将其转换为QWen规定的json数据格式 即：

```
[
    {
        "type": "chatml",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the role of Political Action Committees (PACs) in the American political system?"},
            {"role": "assistant", "content": "政治行动委员会（PACs）在美国政治体系中扮演什么角色？"},
            {"role": "user", "content": "Political Action Committees (PACs) play a significant role in the American political system by providing a legal mechanism for individuals and organizations to pool their financial resources and contribute to political campaigns."},
            {"role": "assistant", "content": "政治行动委员会（PACs）在美国政治体系中发挥着重要作用，为个人和组织提供了一个合法机制，以汇集他们的财务资源并为政治竞选提供资金。"}
        ],
        "source": "self-made"
    }
]
```

这种格式的数据，然后再生成train和val（200）

最后使用 /root/data/wjy/vip_vul_pro/Fine_tune/SafeMTData/Attack_600.json 600个数据进行测试

（通用预料库：

1. [GitHub - Instruction-Tuning-with-GPT-4/GPT-4-LLM: Instruction Tuning with GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
2. [shibing624/sharegpt_gpt4 at main](https://huggingface.co/datasets/shibing624/sharegpt_gpt4/tree/main)
3. [hfl/ruozhiba_gpt4 at main](https://huggingface.co/datasets/hfl/ruozhiba_gpt4/tree/main)

将上述数据库中含有gpt 或 chatgpt字段的都删掉，然后保存新数据）

002 使用GPT4生成对抗样本prompt，注意事项：由于GPT4对齐太好，所以选择对齐性能较差的模型做 ClueAI/ChatYuan-large-v1 。

**什么是对抗样本prompt？即使用生成的提示词，来对大模型的安全性进行评估，看会不会通过一些所谓的技巧或者在特定的对话场景下，来诱使大模型输出违规（不一定违法）的内容。**



### 2024.12.9-2024.12.13

001 完成大模型对抗样本prompt生成。

002 对微调的大模型进行测试和部署（10.0.81.159）model=safe_v3。测试内容分别是自我认知和安全对齐两个方面，并完成微调大模型的测试报告。

003 完成对《基于图数据库与图神经网络识别关键基础设施网络资产 》专利的修改。

### 2024.12.16-2024.12.20

001 完成对《基于图数据库与图神经网络识别关键基础设施网络资产 》专利的修改与专利专家的对接。

002 大模型+VIP漏洞情报工作，如果在漏洞情报描述提取的CPE，和已有的向量数据库匹配分数较低，则新增加“建议返回的新数据”。

003  产品部门的新要求：返回所有特定需求的版本号 例如返回 如下形式的 （不通过向量相似度检索，直接匹配他们的数据库？）

```json
{
    "index": 1,
    "part": "a",
    "vendor": "phpgroupware",
    "product": "phpgroupware",
    "version": [0.9、0.1、0.3],
    "update": "",
    "edition": "",
    "language": "",
    "score": 97.98
  }
```

### 2024.12.23- 2024.12.27

001 完成大模型自我认知项目相关文档的撰写，《开发规范文档》、《用户接口文档》、《部署文档》、《技术架构设计文档》。

002 对VIP漏洞情报项目进行持续的跟进和改善，通过对向量检索结果进行排序然后再使用大模型prompt对cpe版本号进行筛选。

### 2024.12.30-2025.1.3

001完成对漏洞情报项目的version版本筛选，新增选择大模型种类功能，用户调用API接口时需要输入相关的api_key以及base_url，并与产品经理进行对接。
002 为api接口添加中文描述，设置flag返回向量数据库中没有的cpe数据（标记为大模型生成的cpe）,现在的问题是json格式有问题，其中是单引号 。



```
对于上述的json格式的相关报错已解决，并不是格式的问题，是在 FastAPI 中，`@app.post("/search_cpe", response_model=List[CPESearchResult])` 这一行是一个路由装饰器，用于定义一个 HTTP POST 请求的处理函数，并指定该请求的返回值应符合特定的 **Pydantic 数据模型**，我在新的返回中是返回的字典格式，但是在路由装饰器中返回的却是list数据类型，所以有问题。
```



### 2025.1.6-2025.1.10

001 完成对api接口的flag，即返回向量数据库中没有的cpe数据（标记为大模型生成的cpe），若向量数据库中没有，则返回[]，并建议添加到数据库中。

002 完成gradio展示，分四个部分，首先是01输入漏洞情报，02展示大模型提取的cpe字段，03经过版本号排序的top50的search结果，04展示再次使用大模型筛选出来的符合版本号限制的CPE字段。

#### 各个大模型API_Key

~~deep_seek_v3(失效了)~~

```python
model_name = 'deepseek-chat'
base_url = 'https://api.deepseek.com/v1'
api_key = 'sk-a6795bbb3131498d8d1dedd46b04a2a0'
```

gpt-4o

```python
model_name = 'gpt-4o-2024-08-06'
base_url = 'http://43.154.251.242:10050/v1'
api_key = 'sk-KJziv58j8TxW2j895f53B46408E04311Ac4121FcCa8f98Eb'
```

glm-4-plus

```python
model_name = 'glm-4-plus'
base_url = 'https://open.bigmodel.cn/api/paas/v4/'
api_key = '40fb89ca21222f4bcef79d814d2dfeab.CPDlP2b8CL2W0qeb'
```

qwen_max

```python
model_name = 'qwen-max-latest'
base_url = 'https://dashscope.aliyuncs.com/compatible-mode/v1'
api_key = 'sk-75bb6015f0d249408148a2bc37620525'
```

### 2025.1.13-2025.1.17

没干啥 


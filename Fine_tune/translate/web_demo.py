
from argparse import ArgumentParser
from threading import Thread

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

DEFAULT_CKPT_PATH = "/data/huggingface_models/safe_v3_checkpoint60" #æ›´æ”¹æ¨¡å‹è·¯å¾„



def _get_args():
    """
        è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºå¯åŠ¨èŠå¤©æœºå™¨äººç¨‹åºã€‚
        å®¢æˆ·å¯ä»¥é€šè¿‡è¿™äº›å‚æ•°è‡ªå®šä¹‰è¿è¡Œçš„é…ç½®ï¼Œå¦‚æ¨¡å‹è·¯å¾„ã€æ˜¯å¦ä½¿ç”¨CPUç­‰ã€‚
        """
    parser = ArgumentParser(description="Appsec bot web chat demo.")
    parser.add_argument(
        "-c",
        "--checkpoint-path",
        type=str,
        default=DEFAULT_CKPT_PATH,
        help="Checkpoint name or path, default to %(default)r",
    )
    parser.add_argument(
        "--cpu-only", action="store_true", help="Run demo with CPU only"
    )

    parser.add_argument(
        "--share",
        action="store_true",
        default=False,
        help="Create a publicly shareable link for the interface.",
    )
    parser.add_argument(
        "--inbrowser",
        action="store_true",
        default=False,
        help="Automatically launch the interface in a new tab on the default browser.",
    )
    parser.add_argument(
        "--server-port", type=int, default=58001, help="Demo server port."
    )
    parser.add_argument(
        "--server-name", type=str, default="0.0.0.0", help="Demo server name."
    )

    args = parser.parse_args()
    return args


def _load_model_tokenizer(args):
    """
       åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç”¨äºæ”¯æŒè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆã€‚
       æ ¹æ®ç”¨æˆ·çš„è®¾å¤‡é…ç½®ï¼Œæ”¯æŒä½¿ç”¨CPUæˆ–GPUè¿è¡Œã€‚
       """
    tokenizer = AutoTokenizer.from_pretrained(
        args.checkpoint_path,
        resume_download=True,
    )

    if args.cpu_only:
        device_map = "cpu"
    else:
        device_map = "auto"

    model = AutoModelForCausalLM.from_pretrained(
        args.checkpoint_path,
        torch_dtype="auto",
        device_map=device_map,
        resume_download=True,
    ).eval()
    model.generation_config.max_new_tokens = 2048  # For chat.

    return model, tokenizer

# èŠå¤©åŠŸèƒ½å®ç°ï¼ˆæµå¼ç”Ÿæˆï¼‰
def _chat_stream(model, tokenizer, query, history):
    """
        æµå¼ç”ŸæˆèŠå¤©æœºå™¨äººçš„å›ç­”ã€‚
        ç”¨æˆ·çš„è¾“å…¥å’Œå†å²è®°å½•ä¼šè¢«æ‹¼æ¥åé€å…¥æ¨¡å‹ï¼Œæ¨¡å‹é€æ­¥è¿”å›ç”Ÿæˆçš„å›ç­”ã€‚
        """
    conversation = []
    for query_h, response_h in history:
        conversation.append({"role": "user", "content": query_h})
        conversation.append({"role": "assistant", "content": response_h})
    conversation.append({"role": "user", "content": query}) # æ·»åŠ å½“å‰ç”¨æˆ·çš„æé—®
    input_text = tokenizer.apply_chat_template(
        conversation,
        add_generation_prompt=True, # è‡ªåŠ¨æ·»åŠ ç”Ÿæˆæç¤º
        tokenize=False, # ä¸æå‰åˆ†è¯
    )
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
    inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
    streamer = TextIteratorStreamer(
        tokenizer=tokenizer, skip_prompt=True, timeout=60.0, skip_special_tokens=True
    )
    # è®¾ç½®ç”Ÿæˆé…ç½®
    generation_kwargs = {
        **inputs,
        "streamer": streamer,
    }
    # é€šè¿‡çº¿ç¨‹è¿è¡Œç”Ÿæˆï¼Œé˜²æ­¢ç•Œé¢å¡é¡¿
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    # å®æ—¶è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
    for new_text in streamer:
        yield new_text


def _gc():
    import gc
    """
       æ¸…ç†å†…å­˜å’Œæ˜¾å­˜ï¼Œä»¥é˜²æ­¢æ˜¾å­˜æº¢å‡ºæˆ–å†…å­˜å ç”¨è¿‡é«˜ã€‚
       """
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def _launch_demo(args, model, tokenizer):
    """
        ä½¿ç”¨Gradioåˆ›å»ºèŠå¤©æœºå™¨äººWebç•Œé¢ï¼Œå¹¶æ ¹æ®ç”¨æˆ·æ“ä½œå¤„ç†èŠå¤©é€»è¾‘ã€‚
        """
    def predict(_query, _chatbot, _task_history):
        """
                å¤„ç†ç”¨æˆ·çš„æé—®å¹¶è¿”å›æœºå™¨äººç”Ÿæˆçš„å›ç­”ã€‚
                æ”¯æŒæµå¼è¾“å‡ºï¼Œå›ç­”ä¼šé€æ­¥æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šã€‚
                """
        print(f"User: {_query}")
        _chatbot.append((_query, ""))
        full_response = ""
        response = ""
        for new_text in _chat_stream(model, tokenizer, _query, history=_task_history):
            response += new_text
            _chatbot[-1] = (_query, response)

            yield _chatbot
            full_response = response

        print(f"History: {_task_history}")
        _task_history.append((_query, full_response))
        print(f"Appsec bot: {full_response}")

    def regenerate(_chatbot, _task_history):
        if not _task_history:
            yield _chatbot
            return
        item = _task_history.pop(-1)
        _chatbot.pop(-1)
        yield from predict(item[0], _chatbot, _task_history)

    def reset_user_input():
        return gr.update(value="")

    def reset_state(_chatbot, _task_history):
        _task_history.clear()
        _chatbot.clear()
        _gc()
        return _chatbot

    with gr.Blocks(css="footer{display:none !important}") as demo:
        gr.Markdown(
            """\
<center><font size=3>This WebUI is based on Appsec bot, developed by è½¯è¯„ä¸­å¿ƒ. \
(æœ¬WebUIåŸºäºAppsec botæ‰“é€ ï¼Œå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚)</center>"""
        )

        chatbot = gr.Chatbot(label="Appsec bot", elem_classes="control-height")
        query = gr.Textbox(lines=2, label="Input")
        task_history = gr.State([])

        with gr.Row():
            empty_btn = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
            submit_btn = gr.Button("ğŸš€ Submit (å‘é€)")
            regen_btn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")

        submit_btn.click(
            predict, [query, chatbot, task_history], [chatbot], show_progress=True
        )
        submit_btn.click(reset_user_input, [], [query])
        empty_btn.click(
            reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True
        )
        regen_btn.click(
            regenerate, [chatbot, task_history], [chatbot], show_progress=True
        )

        gr.Markdown("""\
<font size=2>Note: This demo is governed by the original license of Appsec bot. \
We strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \
including hate speech, violence, pornography, deception, etc. \
(æ³¨ï¼šæœ¬æ¼”ç¤ºå—Appsec botçš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\
åŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)""")

    demo.queue().launch(
        share=args.share,
        inbrowser=args.inbrowser,
        server_port=args.server_port,
        server_name=args.server_name,
        # show_api=False,
        quiet=True,  # éšè—å¯åŠ¨ä¿¡æ¯
        # show_tips=False,  # éšè—æç¤ºä¿¡æ¯
        # show_api=False,  # éšè— API æ ‡å¿—
    )


def main():
    """
        ä¸»å‡½æ•°ï¼šè§£æå‚æ•°ã€åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ï¼Œå¹¶å¯åŠ¨èŠå¤©æœºå™¨äººç•Œé¢ã€‚
        """
    args = _get_args()# è·å–å‘½ä»¤è¡Œå‚æ•°

    model, tokenizer = _load_model_tokenizer(args) # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨

    _launch_demo(args, model, tokenizer) # å¯åŠ¨èŠå¤©ç•Œé¢


if __name__ == "__main__":
    main()

